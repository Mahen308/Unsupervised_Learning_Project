{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738e1e29-c5ff-4f2e-8912-0722ffa4aacc",
   "metadata": {},
   "source": [
    "***Setup and imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec98d6d8-e6d5-4598-b89a-0319fff2a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python & scientific stack\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "# Sparse + ML utilities\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec8fd1-a4e6-41d3-8e3f-cf80ac710dd7",
   "metadata": {},
   "source": [
    "***Configuration & Data Loading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077842bc-3983-419a-8861-24aec68d8877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (5703555, 3), Test size: (633686, 2)\n",
      "Rating range: [1.0, 10.0], Global mean=7.809\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_DIR = \"Data\"\n",
    "\n",
    "PATH_ANIME = os.path.join(BASE_DIR, \"anime.csv\")\n",
    "PATH_TRAIN = os.path.join(BASE_DIR, \"train.csv\")\n",
    "PATH_TEST  = os.path.join(BASE_DIR, \"test.csv\")\n",
    "\n",
    "USER, ITEM, RATE = \"user_id\", \"anime_id\", \"rating\"\n",
    "\n",
    "# Speed mode toggle\n",
    "USE_SMALL = False\n",
    "USER_CAP  = 2000\n",
    "ITEM_CAP  = 3000\n",
    "\n",
    "# Load tables\n",
    "anime_raw = pd.read_csv(PATH_ANIME)\n",
    "train_raw = pd.read_csv(PATH_TRAIN)\n",
    "test_raw  = pd.read_csv(PATH_TEST)\n",
    "\n",
    "# Clean invalid ratings\n",
    "train_raw = train_raw[pd.to_numeric(train_raw[RATE], errors=\"coerce\").notnull()].copy()\n",
    "train_raw[RATE] = train_raw[RATE].astype(float)\n",
    "train_raw = train_raw[train_raw[RATE] >= 0].copy()\n",
    "\n",
    "# Stats\n",
    "rmin, rmax = float(train_raw[RATE].min()), float(train_raw[RATE].max())\n",
    "gmean = float(train_raw[RATE].mean())\n",
    "\n",
    "print(f\"Train size: {train_raw.shape}, Test size: {test_raw.shape}\")\n",
    "print(f\"Rating range: [{rmin}, {rmax}], Global mean={gmean:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3f3d9-48fe-4908-8d38-53a5e0988c4d",
   "metadata": {},
   "source": [
    "***Build Lightweight Content Text (Metadata)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9307ad-74f2-42f3-a3e1-b0a829970378",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = [c for c in [\"genre\", \"type\", \"name\", \"episodes\"] if c in anime_raw.columns]\n",
    "\n",
    "anime_raw[\"meta_text\"] = (\n",
    "    anime_raw[text_cols].astype(str).agg(\" \".join, axis=1)\n",
    ")\n",
    "\n",
    "# Keep only item IDs present in training data\n",
    "all_item_ids = sorted(train_raw[ITEM].unique())\n",
    "anime_info = pd.DataFrame({ITEM: all_item_ids}).merge(\n",
    "    anime_raw[[ITEM, \"meta_text\"]], on=ITEM, how=\"left\"\n",
    ").fillna({\"meta_text\": \"\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577a26c-680e-4c87-9aa3-a8764d1c3b1c",
   "metadata": {},
   "source": [
    "***Integer Mappings & Sparse Matrix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6696ab04-37cd-4c64-9f0c-c657fc6963f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users=69481, Items=9838\n"
     ]
    }
   ],
   "source": [
    "# ID → index mappings\n",
    "u_index = {u: i for i, u in enumerate(sorted(train_raw[USER].unique()))}\n",
    "i_index = {a: j for j, a in enumerate(all_item_ids)}\n",
    "\n",
    "nU, nI = len(u_index), len(i_index)\n",
    "print(f\"Users={nU}, Items={nI}\")\n",
    "\n",
    "# Sparse CSR rating matrix\n",
    "r = train_raw\n",
    "row = r[USER].map(u_index).values\n",
    "col = r[ITEM].map(i_index).values\n",
    "val = r[RATE].values\n",
    "\n",
    "Rmat = sparse.coo_matrix((val, (row, col)), shape=(nU, nI)).tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45475b8-ed88-43c0-88f0-e95e6b01f7c1",
   "metadata": {},
   "source": [
    "***Per-User & Per-Item Means***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05dc4923-e232-4973-937e-2fd98cd96165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_axis_mean(mat: sparse.csr_matrix, axis=1):\n",
    "    # axis=1 → per-row; axis=0 → per-column\n",
    "    if axis == 0:\n",
    "        return csr_axis_mean(mat.T, axis=1)\n",
    "\n",
    "    means = np.zeros(mat.shape[0], dtype=float)\n",
    "    for r in range(mat.shape[0]):\n",
    "        s, e = mat.indptr[r], mat.indptr[r+1]\n",
    "        means[r] = mat.data[s:e].mean() if e > s else gmean\n",
    "    return means\n",
    "\n",
    "u_mean = csr_axis_mean(Rmat, axis=1)\n",
    "i_mean = csr_axis_mean(Rmat, axis=0)\n",
    "\n",
    "# Quick lookup dict of items per user\n",
    "user_r: List[Dict[int, float]] = [dict() for _ in range(nU)]\n",
    "\n",
    "for u in range(nU):\n",
    "    s, e = Rmat.indptr[u], Rmat.indptr[u+1]\n",
    "    rated_items = Rmat.indices[s:e]\n",
    "    rated_vals  = Rmat.data[s:e]\n",
    "    user_r[u]   = {int(i): float(r) for i, r in zip(rated_items, rated_vals)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da10669-7301-4fa3-a6ec-e969bef0f86c",
   "metadata": {},
   "source": [
    "***Similarity Neighbors (CF + Content)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7474c78c-fa3b-4406-9384-745ce0532404",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NEIGH = 30 if not USE_SMALL else 25\n",
    "\n",
    "# ---- CF neighbor model (item-item) ----\n",
    "item_knn = NearestNeighbors(\n",
    "    metric=\"cosine\", algorithm=\"brute\",\n",
    "    n_neighbors=min(K_NEIGH+1, nI)\n",
    ").fit(Rmat.T)\n",
    "\n",
    "cf_d, cf_i = item_knn.kneighbors(Rmat.T)\n",
    "cf_s = 1.0 - cf_d\n",
    "cf_i, cf_s = cf_i[:, 1:], cf_s[:, 1:]    # remove self-neighbor\n",
    "\n",
    "# ---- Content-based neighbors ----\n",
    "tfidf_vec = TfidfVectorizer(\n",
    "    max_features=40000 if not USE_SMALL else 20000,\n",
    "    min_df=3, ngram_range=(1,2)\n",
    ")\n",
    "content_matrix = tfidf_vec.fit_transform(anime_info[\"meta_text\"])\n",
    "\n",
    "cb_knn = NearestNeighbors(\n",
    "    metric=\"cosine\", algorithm=\"brute\",\n",
    "    n_neighbors=min(K_NEIGH+1, nI)\n",
    ").fit(content_matrix)\n",
    "\n",
    "cb_d, cb_i = cb_knn.kneighbors(content_matrix)\n",
    "cb_s = 1.0 - cb_d\n",
    "cb_i, cb_s = cb_i[:, 1:], cb_s[:, 1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b105c18-7727-49b6-9694-738f677892e6",
   "metadata": {},
   "source": [
    "***Prediction Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926c8107-f448-44b4-9622-1eab0c104a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_neighbor_mean(\n",
    "    uid: int, iid: int,\n",
    "    neigh_idx: np.ndarray,\n",
    "    neigh_sim: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"Predict rating using weighted average on similar items.\"\"\"\n",
    "    total, weight = 0.0, 0.0\n",
    "    rated = user_r[uid]\n",
    "\n",
    "    for nb, s in zip(neigh_idx[iid], neigh_sim[iid]):\n",
    "        r = rated.get(int(nb))\n",
    "        if r is None:\n",
    "            continue\n",
    "        total += s * r\n",
    "        weight += abs(s)\n",
    "\n",
    "    if weight > 0:\n",
    "        return total / weight\n",
    "\n",
    "    return 0.5 * u_mean[uid] + 0.5 * i_mean[iid]\n",
    "\n",
    "\n",
    "def hybrid_predict(uid: int, iid: int, alpha: float) -> float:\n",
    "    p_cf = weighted_neighbor_mean(uid, iid, cf_i, cf_s)\n",
    "    p_cb = weighted_neighbor_mean(uid, iid, cb_i, cb_s)\n",
    "    return alpha * p_cf + (1 - alpha) * p_cb\n",
    "\n",
    "\n",
    "def predict(uid, iid, alpha):\n",
    "    u = u_index.get(uid)\n",
    "    i = i_index.get(iid)\n",
    "\n",
    "    if u is None and i is None:\n",
    "        return gmean\n",
    "    if u is None:\n",
    "        return i_mean[i]\n",
    "    if i is None:\n",
    "        return u_mean[u]\n",
    "\n",
    "    p = hybrid_predict(u, i, alpha)\n",
    "    return float(np.clip(p, rmin, rmax))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112eb22f-c049-41bf-9d9a-0f4c75043fa8",
   "metadata": {},
   "source": [
    "***Tune α on Validation Set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3612d00-8f70-4898-9fe8-2fbfcbb75293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal α = 0.70 | RMSE=1.1931\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>1.193146</td>\n",
       "      <td>0.871180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.194525</td>\n",
       "      <td>0.871917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>1.198820</td>\n",
       "      <td>0.874706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1.202931</td>\n",
       "      <td>0.877067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.211448</td>\n",
       "      <td>0.882437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.218221</td>\n",
       "      <td>0.886259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.230815</td>\n",
       "      <td>0.894289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>1.256610</td>\n",
       "      <td>0.909699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.288446</td>\n",
       "      <td>0.928118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.325890</td>\n",
       "      <td>0.948891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.368480</td>\n",
       "      <td>0.971492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha      rmse       mae\n",
       "7     0.7  1.193146  0.871180\n",
       "8     0.8  1.194525  0.871917\n",
       "6     0.6  1.198820  0.874706\n",
       "9     0.9  1.202931  0.877067\n",
       "5     0.5  1.211448  0.882437\n",
       "10    1.0  1.218221  0.886259\n",
       "4     0.4  1.230815  0.894289\n",
       "3     0.3  1.256610  0.909699\n",
       "2     0.2  1.288446  0.928118\n",
       "1     0.1  1.325890  0.948891\n",
       "0     0.0  1.368480  0.971492"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reliable RMSE\n",
    "def RMSE(true, pred):\n",
    "    try:\n",
    "        return mean_squared_error(true, pred, squared=False)\n",
    "    except:\n",
    "        return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "# 90/10 split\n",
    "train_small, valid = train_test_split(train_raw, test_size=0.10, random_state=13)\n",
    "\n",
    "# Downsample for speed\n",
    "valid = valid.sample(n=min(50000, len(valid)), random_state=13).reset_index(drop=True)\n",
    "\n",
    "def evaluate(df, alpha):\n",
    "    true = df[RATE].astype(float).values\n",
    "    pred = [predict(u, a, alpha) for u, a in df[[USER, ITEM]].itertuples(index=False)]\n",
    "\n",
    "    return RMSE(true, pred), mean_absolute_error(true, pred)\n",
    "\n",
    "alphas = np.linspace(0, 1, 11)\n",
    "results = []\n",
    "\n",
    "best_alpha = None\n",
    "best_rmse = float(\"inf\")\n",
    "\n",
    "for a in alphas:\n",
    "    rm, ma = evaluate(valid, a)\n",
    "    results.append((a, rm, ma))\n",
    "\n",
    "    if rm < best_rmse:\n",
    "        best_rmse = rm\n",
    "        best_alpha = a\n",
    "\n",
    "print(f\"Optimal α = {best_alpha:.2f} | RMSE={best_rmse:.4f}\")\n",
    "pd.DataFrame(results, columns=[\"alpha\", \"rmse\", \"mae\"]).sort_values(\"rmse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6f1575-7142-4b6d-8453-9efb3aa30d7a",
   "metadata": {},
   "source": [
    "***Predict for Test Set***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a9006c9-6aeb-4697-a7fb-d4d06660ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Data\\predictions_custom.csv\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "CHUNK = 200_000\n",
    "\n",
    "for s in range(0, len(test_raw), CHUNK):\n",
    "    block = test_raw.iloc[s:s+CHUNK]\n",
    "    block_pred = [\n",
    "        predict(u, i, best_alpha)\n",
    "        for u, i in block[[USER, ITEM]].itertuples(index=False)\n",
    "    ]\n",
    "    predictions.extend(block_pred)\n",
    "\n",
    "submission = test_raw.copy()\n",
    "submission[RATE] = predictions[:len(test_raw)]\n",
    "\n",
    "OUT_CSV = os.path.join(BASE_DIR, \"predictions_custom.csv\")\n",
    "submission.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a5f92-bc3e-443a-b71d-4ada70d0ab22",
   "metadata": {},
   "source": [
    "***Save Model Artifact***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459addb0-ed35-4530-8923-556c2f44519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact saved → Data\\hybrid_recommender_custom.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_py(x):\n",
    "    \"\"\"\n",
    "    Convert numpy types → Python native types so JSON can serialize them.\n",
    "    \"\"\"\n",
    "    if isinstance(x, (np.integer, np.int64, np.int32)):\n",
    "        return int(x)\n",
    "    if isinstance(x, (np.floating, np.float32, np.float64)):\n",
    "        return float(x)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    return x\n",
    "\n",
    "def pack_csr_safe(csr):\n",
    "    return {\n",
    "        \"data\":   to_py(csr.data),\n",
    "        \"indices\": to_py(csr.indices),\n",
    "        \"indptr\":  to_py(csr.indptr),\n",
    "        \"shape\":   [int(csr.shape[0]), int(csr.shape[1])]\n",
    "    }\n",
    "\n",
    "artifact = {\n",
    "    \"global_mean\": float(gmean),\n",
    "    \"rating_min\":  float(rmin),\n",
    "    \"rating_max\":  float(rmax),\n",
    "    \"best_alpha\":  float(best_alpha),\n",
    "    \"user_ids\":    [int(x) for x in u_index.keys()],\n",
    "    \"item_ids\":    [int(x) for x in i_index.keys()],\n",
    "    \"user_mean\":   [float(v) for v in u_mean.tolist()],\n",
    "    \"item_mean\":   [float(v) for v in i_mean.tolist()],\n",
    "    \"R_matrix\":    pack_csr_safe(Rmat),\n",
    "}\n",
    "\n",
    "ART_PATH = os.path.join(BASE_DIR, \"hybrid_recommender_custom.json\")\n",
    "\n",
    "with open(ART_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(artifact, f)\n",
    "\n",
    "print(\"Artifact saved →\", ART_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1310efeb-8e80-4df9-bb6f-fe8a969d5124",
   "metadata": {},
   "source": [
    "***Streamlit***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544c6df9-a8fb-4dce-bae2-670b2ec64934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.51.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (8.2.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<26,>=20 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<13,>=7.1.0 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (6.32.1)\n",
      "Collecting pyarrow<22,>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\envs\\creating_an_environment\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Downloading streamlit-1.51.0-py3-none-any.whl (10.2 MB)\n",
      "   ---------------------------------------- 0.0/10.2 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.1/10.2 MB 16.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.2 MB 19.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.2 MB 17.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.2/10.2 MB 16.5 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 731.2/731.2 kB 16.3 MB/s eta 0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl (26.1 MB)\n",
      "   ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.9/26.1 MB 17.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.0/26.1 MB 14.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 8.1/26.1 MB 12.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.7/26.1 MB 12.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.1/26.1 MB 12.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.7/26.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.4/26.1 MB 12.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.0/26.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.6/26.1 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.0/26.1 MB 12.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.1/26.1 MB 12.2 MB/s eta 0:00:00\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 3.4/6.9 MB 17.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.3/6.9 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.9/6.9 MB 14.0 MB/s eta 0:00:00\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Installing collected packages: watchdog, toml, tenacity, smmap, pyarrow, cachetools, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "\n",
      "   --- ------------------------------------  1/12 [toml]\n",
      "   ---------- -----------------------------  3/12 [smmap]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ------------- --------------------------  4/12 [pyarrow]\n",
      "   ----------------------- ----------------  7/12 [pydeck]\n",
      "   ----------------------- ----------------  7/12 [pydeck]\n",
      "   -------------------------- -------------  8/12 [gitdb]\n",
      "   ------------------------------ ---------  9/12 [gitpython]\n",
      "   ------------------------------ ---------  9/12 [gitpython]\n",
      "   --------------------------------- ------ 10/12 [altair]\n",
      "   --------------------------------- ------ 10/12 [altair]\n",
      "   --------------------------------- ------ 10/12 [altair]\n",
      "   --------------------------------- ------ 10/12 [altair]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ------------------------------------ --- 11/12 [streamlit]\n",
      "   ---------------------------------------- 12/12 [streamlit]\n",
      "\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 cachetools-6.2.2 gitdb-4.0.12 gitpython-3.1.45 pyarrow-21.0.0 pydeck-0.9.1 smmap-5.0.2 streamlit-1.51.0 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9960589f-cea5-461a-98dc-227c69e32d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mneighbors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# CONFIG - adjust paths\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     17\u001b[39m DATA_DIR = \u001b[33m\"\u001b[39m\u001b[33mData\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "# app.py - Option A: simple user+anime prediction UI\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG - adjust paths\n",
    "# -----------------------\n",
    "DATA_DIR = \"Data\"\n",
    "ANIME_PATH = os.path.join(DATA_DIR, \"anime.csv\")\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train.csv\")\n",
    "ARTIFACT_PATH = os.path.join(DATA_DIR, \"hybrid_recommender_custom.json\")\n",
    "\n",
    "# neighbor K (should match what you used previously)\n",
    "K_NEIGH = 30\n",
    "\n",
    "# -----------------------\n",
    "# UTIL: safe json loader (handles numeric lists)\n",
    "# -----------------------\n",
    "def load_json_safe(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# -----------------------\n",
    "# REBUILD minimal model objects\n",
    "# -----------------------\n",
    "@st.cache_data(show_spinner=False)\n",
    "def build_models(anime_csv: str, train_csv: str, artifact_json: str):\n",
    "    anime_df = pd.read_csv(anime_csv)\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    art = load_json_safe(artifact_json)\n",
    "\n",
    "    # basics from artifact (fallbacks used defensively)\n",
    "    global_mean = float(art.get(\"global_mean\", np.mean(art.get(\"user_mean\", [7.5]))))\n",
    "    rating_min = float(art.get(\"rating_min\", 1.0))\n",
    "    rating_max = float(art.get(\"rating_max\", 10.0))\n",
    "    best_alpha = float(art.get(\"best_alpha\", 0.7))\n",
    "\n",
    "    user_ids = [int(x) for x in art[\"user_ids\"]]\n",
    "    item_ids = [int(x) for x in art[\"item_ids\"]]\n",
    "\n",
    "    user_to_index = {uid: i for i, uid in enumerate(user_ids)}\n",
    "    item_to_index = {iid: i for i, iid in enumerate(item_ids)}\n",
    "    index_to_item = {i: iid for iid, i in item_to_index.items()}\n",
    "\n",
    "    n_users, n_items = len(user_ids), len(item_ids)\n",
    "\n",
    "    # rebuild R matrix from artifact if present\n",
    "    if \"R_matrix\" in art and art[\"R_matrix\"] is not None:\n",
    "        pack = art[\"R_matrix\"]\n",
    "        data = np.array(pack[\"data\"], dtype=float)\n",
    "        indices = np.array(pack[\"indices\"], dtype=int)\n",
    "        indptr = np.array(pack[\"indptr\"], dtype=int)\n",
    "        shape = tuple(pack[\"shape\"])\n",
    "        Rmat = sparse.csr_matrix((data, indices, indptr), shape=shape)\n",
    "    else:\n",
    "        # fallback reconstruct from train.csv\n",
    "        tri = train_df[train_df[\"anime_id\"].isin(item_ids)].copy()\n",
    "        rows = tri[\"user_id\"].map(user_to_index).dropna().astype(int).values\n",
    "        cols = tri[\"anime_id\"].map(item_to_index).dropna().astype(int).values\n",
    "        vals = tri[\"rating\"].astype(float).values\n",
    "        Rmat = sparse.coo_matrix((vals, (rows, cols)), shape=(n_users, n_items)).tocsr()\n",
    "\n",
    "    # per-user and per-item means\n",
    "    def csr_row_means(csr: sparse.csr_matrix):\n",
    "        arr = np.zeros(csr.shape[0], dtype=float)\n",
    "        for r in range(csr.shape[0]):\n",
    "            s, e = csr.indptr[r], csr.indptr[r+1]\n",
    "            arr[r] = float(csr.data[s:e].mean()) if (e > s) else float(global_mean)\n",
    "        return arr\n",
    "\n",
    "    user_mean = csr_row_means(Rmat)\n",
    "    item_mean = csr_row_means(Rmat.T.tocsr())\n",
    "\n",
    "    # quick per-user rating dict\n",
    "    user_rdict: List[Dict[int, float]] = [dict() for _ in range(n_users)]\n",
    "    for u in range(n_users):\n",
    "        s, e = Rmat.indptr[u], Rmat.indptr[u+1]\n",
    "        idxs, vals = Rmat.indices[s:e], Rmat.data[s:e]\n",
    "        user_rdict[u] = {int(i): float(v) for i, v in zip(idxs, vals)}\n",
    "\n",
    "    # prepare content TF-IDF using anime metadata and the item_ids order\n",
    "    meta_cols = [c for c in [\"genre\", \"type\", \"name\", \"episodes\", \"synopsis\"] if c in anime_df.columns]\n",
    "    if len(meta_cols) > 0:\n",
    "        anime_df[\"meta_text\"] = anime_df[meta_cols].astype(str).agg(\" \".join, axis=1)\n",
    "    else:\n",
    "        anime_df[\"meta_text\"] = \"\"\n",
    "\n",
    "    # build anime info frame in the order of item_ids\n",
    "    info = pd.DataFrame({\"anime_id\": item_ids})\n",
    "    info = info.merge(anime_df.rename(columns={\"anime_id\": \"anime_id\"}), on=\"anime_id\", how=\"left\")\n",
    "    info[\"meta_text\"] = info[\"meta_text\"].fillna(\"\")\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2)\n",
    "    tfidf_item_matrix = tfidf.fit_transform(info[\"meta_text\"].astype(str))\n",
    "\n",
    "    # CF kNN (item-item) on R.T\n",
    "    cf_knn = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\",\n",
    "                              n_neighbors=min(K_NEIGH + 1, n_items))\n",
    "    cf_knn.fit(Rmat.T)\n",
    "    cf_dists, cf_inds = cf_knn.kneighbors(Rmat.T, n_neighbors=min(K_NEIGH + 1, n_items), return_distance=True)\n",
    "    cf_sims = 1.0 - cf_dists\n",
    "    cf_inds = cf_inds[:, 1:]; cf_sims = cf_sims[:, 1:]\n",
    "\n",
    "    # CB kNN on TF-IDF\n",
    "    cb_knn = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\",\n",
    "                              n_neighbors=min(K_NEIGH + 1, n_items))\n",
    "    cb_knn.fit(tfidf_item_matrix)\n",
    "    cb_dists, cb_inds = cb_knn.kneighbors(tfidf_item_matrix, n_neighbors=min(K_NEIGH + 1, n_items), return_distance=True)\n",
    "    cb_sims = 1.0 - cb_dists\n",
    "    cb_inds = cb_inds[:, 1:]; cb_sims = cb_sims[:, 1:]\n",
    "\n",
    "    # name mapping for dropdowns\n",
    "    name_map = {}\n",
    "    for _, row in anime_df.iterrows():\n",
    "        if \"anime_id\" in row and not pd.isna(row[\"anime_id\"]):\n",
    "            try:\n",
    "                aid = int(row[\"anime_id\"])\n",
    "            except:\n",
    "                continue\n",
    "            name_map[aid] = row.get(\"name\", str(aid))\n",
    "\n",
    "    return {\n",
    "        \"artifact\": art,\n",
    "        \"Rmat\": Rmat,\n",
    "        \"user_ids\": user_ids,\n",
    "        \"item_ids\": item_ids,\n",
    "        \"user_to_index\": user_to_index,\n",
    "        \"item_to_index\": item_to_index,\n",
    "        \"index_to_item\": index_to_item,\n",
    "        \"user_mean\": user_mean,\n",
    "        \"item_mean\": item_mean,\n",
    "        \"user_rdict\": user_rdict,\n",
    "        \"cf_inds\": cf_inds,\n",
    "        \"cf_sims\": cf_sims,\n",
    "        \"cb_inds\": cb_inds,\n",
    "        \"cb_sims\": cb_sims,\n",
    "        \"name_map\": name_map,\n",
    "        \"global_mean\": global_mean,\n",
    "        \"rating_min\": rating_min,\n",
    "        \"rating_max\": rating_max,\n",
    "        \"best_alpha\": best_alpha\n",
    "    }\n",
    "\n",
    "# -----------------------\n",
    "# Prediction helpers\n",
    "# -----------------------\n",
    "def weighted_neighbor_mean(uidx: int, iidx: int, neigh_idx: np.ndarray, neigh_sim: np.ndarray, user_rdict) -> float:\n",
    "    numer = 0.0\n",
    "    denom = 0.0\n",
    "    rated = user_rdict[uidx]\n",
    "    for nb, s in zip(neigh_idx[iidx], neigh_sim[iidx]):\n",
    "        r = rated.get(int(nb))\n",
    "        if r is not None:\n",
    "            numer += s * r\n",
    "            denom += abs(s)\n",
    "    if denom > 0:\n",
    "        return float(numer / denom)\n",
    "    return None\n",
    "\n",
    "def hybrid_components(uidx: int, iidx: int, alpha: float, cf_inds, cf_sims, cb_inds, cb_sims, user_mean, item_mean, user_rdict, global_mean):\n",
    "    p_cf = weighted_neighbor_mean(uidx, iidx, cf_inds, cf_sims, user_rdict)\n",
    "    p_cb = weighted_neighbor_mean(uidx, iidx, cb_inds, cb_sims, user_rdict)\n",
    "\n",
    "    # Backoffs if no neighbors\n",
    "    if p_cf is None and p_cb is None:\n",
    "        fallback = 0.5 * user_mean[uidx] + 0.5 * item_mean[iidx]\n",
    "        return fallback, None, None\n",
    "\n",
    "    if p_cf is None:\n",
    "        p_cf = 0.5 * user_mean[uidx] + 0.5 * item_mean[iidx]\n",
    "    if p_cb is None:\n",
    "        p_cb = 0.5 * user_mean[uidx] + 0.5 * item_mean[iidx]\n",
    "\n",
    "    hybrid = float(alpha * p_cf + (1.0 - alpha) * p_cb)\n",
    "    return hybrid, float(p_cf), float(p_cb)\n",
    "\n",
    "# -----------------------\n",
    "# Streamlit UI\n",
    "# -----------------------\n",
    "st.set_page_config(page_title=\"Simple Hybrid Predictor\", layout=\"centered\")\n",
    "st.title(\"Hybrid Recommender — Single prediction (User + Anime)\")\n",
    "\n",
    "with st.spinner(\"Loading models...\"):\n",
    "    state = build_models(ANIME_PATH, TRAIN_PATH, ARTIFACT_PATH)\n",
    "\n",
    "user_ids = state[\"user_ids\"]\n",
    "item_ids = state[\"item_ids\"]\n",
    "user_to_index = state[\"user_to_index\"]\n",
    "item_to_index = state[\"item_to_index\"]\n",
    "index_to_item = state[\"index_to_item\"]\n",
    "user_mean = state[\"user_mean\"]\n",
    "item_mean = state[\"item_mean\"]\n",
    "user_rdict = state[\"user_rdict\"]\n",
    "cf_inds, cf_sims = state[\"cf_inds\"], state[\"cf_sims\"]\n",
    "cb_inds, cb_sims = state[\"cb_inds\"], state[\"cb_sims\"]\n",
    "name_map = state[\"name_map\"]\n",
    "global_mean = state[\"global_mean\"]\n",
    "rating_min = state[\"rating_min\"]\n",
    "rating_max = state[\"rating_max\"]\n",
    "best_alpha = state[\"best_alpha\"]\n",
    "\n",
    "st.markdown(\"### Pick a user and an anime to predict the rating\")\n",
    "\n",
    "# user selection: show a small sample list but allow typing\n",
    "sample_users = user_ids[:200]\n",
    "sel_user = st.selectbox(\"Pick user (from known users)\", options=sample_users)\n",
    "typed_user = st.text_input(\"Or type user id\", value=str(sel_user))\n",
    "try:\n",
    "    chosen_user = int(typed_user)\n",
    "except:\n",
    "    st.error(\"User id must be an integer\")\n",
    "    st.stop()\n",
    "\n",
    "# anime selection: build readable dropdown \"id — name\" for first N, but allow typing\n",
    "display_items = []\n",
    "for aid in item_ids[:3000]:  # cap for dropdown performance\n",
    "    name = name_map.get(aid, str(aid))\n",
    "    display_items.append(f\"{aid} — {name}\")\n",
    "\n",
    "selected_display = st.selectbox(\"Pick anime (from known items)\", options=display_items)\n",
    "# extract id from selected_display which begins with \"<id> —\"\n",
    "selected_aid = int(selected_display.split(\" — \", 1)[0])\n",
    "\n",
    "typed_aid = st.text_input(\"Or type anime id\", value=str(selected_aid))\n",
    "try:\n",
    "    chosen_aid = int(typed_aid)\n",
    "except:\n",
    "    st.error(\"Anime id must be an integer\")\n",
    "    st.stop()\n",
    "\n",
    "# alpha slider (start with best_alpha from artifact but user may override)\n",
    "alpha = st.slider(\"Hybrid weight α (higher = more CF)\", min_value=0.0, max_value=1.0, value=float(best_alpha), step=0.05)\n",
    "\n",
    "# Predict button\n",
    "if st.button(\"Predict rating\"):\n",
    "    # determine indices\n",
    "    uidx = user_to_index.get(chosen_user)\n",
    "    iidx = item_to_index.get(chosen_aid)\n",
    "\n",
    "    if uidx is None and iidx is None:\n",
    "        st.warning(\"Unknown user and unknown anime — returning global mean\")\n",
    "        st.write(f\"Predicted rating: **{global_mean:.3f}**\")\n",
    "    elif uidx is None:\n",
    "        st.warning(\"Unknown user — returning item mean\")\n",
    "        # item mean index might still exist\n",
    "        if iidx is not None:\n",
    "            st.write(f\"Item mean: **{item_mean[iidx]:.3f}**\")\n",
    "        else:\n",
    "            st.write(f\"Global mean: **{global_mean:.3f}**\")\n",
    "    elif iidx is None:\n",
    "        st.warning(\"Unknown anime — returning user mean\")\n",
    "        st.write(f\"User mean: **{user_mean[uidx]:.3f}**\")\n",
    "    else:\n",
    "        # compute hybrid and components\n",
    "        hybrid, p_cf, p_cb = hybrid_components(uidx, iidx, alpha,\n",
    "                                               cf_inds, cf_sims, cb_inds, cb_sims,\n",
    "                                               user_mean, item_mean, user_rdict, global_mean)\n",
    "        # if components missing, show fallback explanation\n",
    "        st.write(\"### Prediction result\")\n",
    "        st.write(f\"Hybrid prediction: **{float(np.clip(hybrid, rating_min, rating_max)):.3f}** (clipped to [{rating_min}, {rating_max}])\")\n",
    "        if p_cf is None and p_cb is None:\n",
    "            st.info(\"No neighbor ratings found for this user on similar items — used backoff to user/item means.\")\n",
    "            st.write(f\"User mean: {user_mean[uidx]:.3f}  —  Item mean: {item_mean[iidx]:.3f}\")\n",
    "        else:\n",
    "            st.write(f\"CF-only prediction: {p_cf:.3f}\" if p_cf is not None else \"CF-only: (no neighbor info)\")\n",
    "            st.write(f\"CB-only prediction: {p_cb:.3f}\" if p_cb is not None else \"CB-only: (no neighbor info)\")\n",
    "            st.write(f\"Alpha used: {alpha:.2f}  → hybrid = α*CF + (1-α)*CB\")\n",
    "\n",
    "st.markdown(\"---\")\n",
    "st.caption(\"This app uses a precomputed artifact and reconstructs item-item (CF) and TF-IDF (CB) neighbors on load.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb1a27-f17f-47f1-a9c6-273375b27896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
